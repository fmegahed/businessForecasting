---
title: "ISA 444: Business Forecasting"
subtitle: "09 - Nonseasonal Smoothing (Cont.)"
author: Fadel M. Megahed
date: 'Spring 2021'
output:
  beamer_presentation:
    number_sections: false
    toc: false
    slide_level: 3
    includes: 
      in_header: structure.txt
classoption: "aspectratio=169"
always_allow_html: yes
bibliography: refs.bib
link-citations: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE,
                      verbose = FALSE,
                      progress = FALSE,
                      fig.align = "center",
                      fig.pos = 'p',
                      fig.width = 5.95,
                      fig.height= 2.5,
                      allowframebreaks = TRUE,
                      fig.margin=TRUE,
                      kable.force.latex = TRUE,
                      cache = FALSE)
options(kableExtra.latex.load_packages = FALSE)
pacman::p_load(kableExtra, xtable, tidyverse, magrittr, tidyquant, fpp2, ggpubr, scales)
```

# Preface

### What we Covered Last Week

\begin{block}{\textbf{Main Learning Outcomes}}
			$\quad$ \textcolor{darkgreen}{\large \checkboxFadel} \textbf{Recognize time series that are appropriate for simple exponential smoothing (SES).} \\
			$\quad$ \textcolor{darkgreen}{\large \checkboxFadel} \textbf{Use SES to smooth past observations of a time series.}
\end{block}

### Recap: A 10,000 Foot View of Forecasting Methods

\vspace{\baselineskip}

\begin{figure}
		\centering
		\adjustbox{max width=\textwidth, frame}{%
			\begin{forest}
				[\large{\textbf{Forecasting}}
				[\textbf{Judgemental}
				[Sales Composite]
				[Customer Survey]
				[Delphi Method]]
				[\textbf{Quantitative}
				[\textcolor{miamired}{\textbf{Extrapolative}}
				[\textcolor{darkgreen}{\textbf{Naive}}]
				[\textbf{Smoothing-based}
				[\textcolor{darkgreen}{\textbf{$\approx$Stationary}}
				[Average]
				[MA]
				[\textcolor{orange}{SES}]]
				[\textcolor{darkgreen}{\textbf{Trend}}
				[\textcolor{orange}{Holt's}]]
				[\textcolor{darkgreen}{\textbf{Both?}}
				[Holt-Winters]
				]]
				[\textbf{``Advanced''}
				[(S)ARIMA]
				[GARCH]]]
				[\textcolor{miamired}{\textbf{Causal}}
				[\textbf{Statistical}
				[(S)ARIMAX]]
				[\textbf{ML-Based}
				[\textcolor{darkgreen}{\textbf{Feature Eng.}}
				[e.g. AutoML]]]
				]
				]
				]
		\end{forest}}
		\caption{A 10,000 foot view of forecasting techniques\footnotemark}
\end{figure}

\footnotetext{An (incomplete) classification of forecasting techniques. Note that these focus on univariate time-series. Hence, they exclude popular approaches used in multivariate time series forecasting.}


### Learning Objectives for Today's Class
\begin{block}{\textbf{Main Learning Outcomes}}
  \begin{itemize}
			\item \textbf{Recognize time series that are appropriate for simple exponential smoothing (SES).}
			\item \textbf{Use SES to smooth past observations of a time series.}
			\item \textbf{Use SES to forecast future observations of a time series.}
			\item \textbf{Compare the forecasting performance of SES to other suitable techniques (i.e., methods that require similar assumptions).}
			\item \textbf{Recognize time series that are appropriate for linear exponential smoothing (LES).}
			\item \textbf{Use LES to forecast future observations of a time series.}
	\end{itemize}
\end{block}


# Simple Exponential Smoothing (SES)

### Recap: Definition and Basic Principles

Simple Exponential Smoothing (SES) is a method used for one-step-ahead forecasting of a time series when there is no trend or seasonal pattern, but the mean may drift slowly over time. The mean is said to have a "local level".

Similar to the idea behind a moving average, SES averages the values of the times series, but applies weights of decreasing importance to values that are farther away from the forecast. The weights of the observations "exponentially decay" as we move away from them in time.

The SES one-step-ahead forecast is given by:
\begin{equation}
  {l}_{t+1} = l_t + \alpha(y_t - l_t ) = \alpha y_t + (1 - \alpha)l_t,
\end{equation}
where $0 < \alpha < 1$ is the smoothing parameter, and $l_t$ is the  level of the series at time $t$. Note that $l_{1+1}$ is often denoted as $f_{t+1}$ since it represents our one step-ahead forecast for $t+1$.


### Recap: Impact of the Smoothing Parameter (Visually)

```{r plotWeight, echo=FALSE}
df = data.frame(t= seq(9, 0, -1), `alpha = 0.2` = rep(NA, 10), `alpha = 0.5` = rep(NA, 10), `alpha = 0.8` = rep(NA, 10))

for (i in 1:nrow(df) ) {
  df$alpha...0.2[i] = 0.2*(1-0.2)^(i-1)
  df$alpha...0.5[i] = 0.5*(1-0.5)^(i-1)
  df$alpha...0.8[i] = 0.8*(1-0.8)^(i-1)
}

df$lag = rev(df$t) + 1 

df %<>% pivot_longer(c(2,3,4))

df %>% ggplot(aes(x = as.integer(lag), y = value, group = name, color = name)) +
  geom_line() + geom_point() + theme_bw() +
  labs(x = 'Lag', y = 'Weight', title = 'Weights on Different Lags in SES', color = 'Smoothing Parameter') +
  theme(legend.position = 'bottom') +
  scale_x_continuous(breaks = scales::pretty_breaks()) +
  scale_color_brewer(type = "qual", palette = 'Dark2')

```

### Example 1: GDP Transformation and SES Smoothing [1]
```{r gdpStationaryA, eval=FALSE}
gdp = tq_get('GDP', get = 'economic.data', from = '1960-01-01') %>% 
  select(-symbol)
gdp %<>% mutate(logGDP = log(price), DeltaLogGDP = logGDP - lag(logGDP))

# Create the three plots
p1 = gdp %>% ggplot(aes(x = date, y = price)) + 
  geom_line() + theme_bw(base_size = 6)
p2 = gdp %>% ggplot(aes(x = date, y = logGDP)) + 
  geom_line() + theme_bw(base_size = 6)
p3 = gdp %>% ggplot(aes(x = date, y = DeltaLogGDP)) + 
  geom_line() + theme_bw(base_size = 6)

# Combining them using the ggpubr package (may need to be installed)
ggpubr::ggarrange(p1, p2, p3, ncol = 1, nrow = 3)
```

### Example 1: GDP Transformation and SES Smoothing [2]

```{r gdpStationaryB, echo=FALSE}
gdp = tq_get('GDP', get = 'economic.data', from = '1960-01-01') %>% 
  select(-symbol)
gdp %<>% mutate(logGDP = log(price), DeltaLogGDP = logGDP - lag(logGDP))

# Create the three plots
p1 = gdp %>% ggplot(aes(x = date, y = price)) + 
  geom_line() + theme_bw(base_size = 6)
p2 = gdp %>% ggplot(aes(x = date, y = logGDP)) + 
  geom_line() + theme_bw(base_size = 6)
p3 = gdp %>% ggplot(aes(x = date, y = DeltaLogGDP)) + 
  geom_line() + theme_bw(base_size = 6)

# Combining them using the ggpubr package (may need to be installed)
ggpubr::ggarrange(p1, p2, p3, ncol = 1, nrow = 3)
```

### Example 1: GDP Transformation and SES Smoothing [3]

\textbf{\textcolor{miamired}{The previous plots show how non-stationary data can be transformed into a more stationary series using the difference in logs.} Note that the log made the series more linear.}

In this quick \textcolor{miamired}{\textbf{live coding session}}, let us compare the forecasting performance of the cumulative average (`dplyr::cummean()`), moving average (`zoo::rollmeanr()`), and the SES (`forecast::ses()`) on the log differenced series. For the purpose of the example, let us use a window size  ($k = 4$), and $\alpha = 0.2$.

\textcolor{miamired}{In class, we will reproduce the results below.}

```{r gdpStationaryForecast, echo=FALSE, results='asis'}
gdp %<>% na.omit() # dropping NA observations (i.e. first obsv)
gdp %<>% mutate(Fcm = cummean(DeltaLogGDP) %>% lag(),
                Fma4 = rollmeanr(DeltaLogGDP, k = 4, na.pad = T) %>% lag,
                Fses = ses(DeltaLogGDP, h = 8, level = 95, 
                           initial = 'simple', alpha = 0.2) %>% .[['fitted']])
res = rbind(accuracy(object = gdp$Fcm, x = gdp$DeltaLogGDP),
            accuracy(object = gdp$Fma4, x = gdp$DeltaLogGDP),
            accuracy(object = gdp$Fses, x = gdp$DeltaLogGDP) )
row.names(res) = c('Cumulative Avg', 'Moving Avg (n=4)', 'SES (alpha=0.2)')
print(xtable(res, align = c('r', rep('c', 5)), digits = c(0, rep(4, 3), 1, 1) ), comment = FALSE)
```



### Training and Validation Samples {.allowframebreaks}

**Often you determine your smoothing parameter based on a training or baseline sample of observations, not the entire series. Then you apply the model using the smoothing parameter to the new observations and evaluate the fit on the out-of-sample observations.**

```{r rollingOrigin, fig.align='center', echo=FALSE, fig.height=1.7}
usdt = tidyquant::tq_get('USDT-USD', from = '2020-11-16', to = '2021-02-14') %>% 
  dplyr::select(date, adjusted)

usdt %>% ggplot(aes(x = date, y = adjusted)) +
  geom_line() + geom_point() +
  scale_x_date(breaks = pretty_breaks(10)) + # beautifying the x-axis
  geom_rect(xmin = usdt$date[1], xmax = usdt$date[70], 
            ymin = min(usdt$adjusted) - 0.0005,
            ymax = max(usdt$adjusted) + 0.0005, alpha = 0.005, fill = 'gray') +
  geom_rect(xmin = usdt$date[71], xmax = usdt$date[91], 
            ymin = min(usdt$adjusted) - 0.0005,
            ymax = max(usdt$adjusted) + 0.0005, alpha = 0.005, fill = 'red') +
  annotate("text", x = usdt$date[40], y = max(usdt$adjusted) + 0.0001,
             color = 'black', label = "Training Region", size = 3, fontface = 'bold') + 
  annotate("text", x = usdt$date[81], y = max(usdt$adjusted) + 0.0001,
             color = 'black', label = "Validation Region", size = 3, fontface = 'bold') + 
  theme_bw(base_size = 7) +
  labs(x = 'Date', y = 'Closing Price', caption = 'Data from 2020-11-16 to  2021-02-14',
       title = 'Depicting Training (~77%) and Validation (~23%) Regions for the USDT-USD Data')

```

  (1) Determine the size of the training, or baseline sample.\footnotemark  
      (a) Training sample size is usually 70-80% of the total available data.  
      (b) Training sample should maintain time order. With time series, the training sample usually consists of observations at the beginning of the sample, while validation sample consists of observations at the end of the available data.  
      
  (2) Select the smoothing parameter based on the observations in the training sample only.  
  
  (3) Evaluate the “in-sample” performance of the forecast using RMSE and graphs using the training sample.  
  
  (4) Apply the model chosen in #2 to the validation sample.  
  
  (5) Evaluate the “out-of-sample” performance of the forecast using RMSE and graphs.


\footnotetext{Slide is based on \href{https://miamioh.edu/fsb/directory/?up=/directory/farmerl2}{Dr. Allison Jones-Farmer's Handouts} for ISA 444, Spring 2020.}



### Optimizing the Smoothing Parameter: WFJ Sales Series
To illustrate the aforementioned concepts, let us examine the data for the [WFJ Sales Example](https://github.com/fmegahed/businessForecasting/blob/master/assignments/WFJ_sales.xlsx?raw=true) (i.e., Example 3.2 in our textbook). Per the textbook example, we will use the first the 26 observations as the estimation sample.

```{r wfjSales}
pacman::p_load(readxl)
download.file("https://github.com/fmegahed/businessForecasting/blob/master/assignments/WFJ_sales.xlsx?raw=true",
              destfile = "Data/WFJ_sales.xlsx", mode = "wb")
WFJ = read_excel("Data/WFJ_sales.xlsx") %>% select(c(1,2))
```

**This example will be coded live in class to obtain the results below.**


```{r wfjSalesComplete, echo=FALSE, results='asis'}
trainData = WFJ[1:26,] # using the first 26 observations for training
ntrain = nrow(trainData) # getting the sample size

sesResults = ses(trainData$`WFJ Sales`, h = 1)
metrics = accuracy(sesResults)
cat(paste0('The optimal alpha obtained using R is equal to ', round(sesResults$model$par['alpha'], 3), '.'), '\n')
print(xtable(metrics, align = c(rep('c', 8)), digits = c(0, rep(3, 7)) ), comment = FALSE)
```

### The Validation Results
**A continuation of the live coding session, where we: (a) examine the validation results; and (b) print the combined training and validation results in one table.**

```{r wfjSalesComplete2, echo=FALSE, results='asis'}
WFJ$sesOpt = ses(WFJ$`WFJ Sales`, h =1, initial = 'simple', alpha = 0.727) %>% .[['fitted']]
validationData = WFJ[27:62,]
validationMetrics = accuracy(object = validationData$sesOpt, x = validationData$`WFJ Sales`)

combinedMetrics = rbind(metrics[1, -c(6,7)], validationMetrics)
row.names(combinedMetrics) = c('Training Set', 'Validation Set')
print(xtable(combinedMetrics, align = c(rep('c', 6)), digits = c(0, rep(3, 5)) ), comment = FALSE)
```

# Linear Exponential Smoothing (LES)

### Definition and Basic Principles {.allowframebreaks}

Linear Exponential Smoothing (LES) is a method used for one-step-ahead forecasting of a time series when there \textcolor{miamired}{is a local trend, but no} seasonal pattern.

A “global” trend occurs when a trend is increasing or decreasing at a nearly constant rate as in a simple linear regression model:
$$
y_t = \beta_0 + \beta_1t + \epsilon_t
$$

**A “local” trend occurs when a linear trend is increasing or decreasing at a nonconstant rate.** LES, also referred to as Holt’s Method or double exponential smoothing, is appropriate when the level ($\beta_0$) of the series is slowly changing as with SES, and the trend is also changing over time.

To compute the **forecast** we will use two smoothing constants, $\alpha$, to smooth the level, and $\beta$, the smoothing constant to smooth the trend.

The estimate of the **level** is: 
\begin{equation}
l_t = \alpha y_t + (1-\alpha)[l_{t-1} + b_{t-1}]
\end{equation}

The estimate of the **trend** is:
\begin{equation}
b_t = \beta [l_t - l_{t-1}] + (1-\beta) b_{t-1}
\end{equation}

To estimate the **point forecast** for time $t+h$ time periods ahead made in time $t$:
\begin{equation}
\hat{y}_{t+h}(t) = l_t + (h\times b_t)
\end{equation}


### What needs to be Determined/Optimized for? [1]

- Starting value for the level, $L_0$  and the starting value of the trend, $B_0$:  

  - When fitting “by hand” you can use a training sample and fit a simple linear trend regression, $\hat{y}_t = b_0 + b_1 t$, to obtain initial estimates of $L_0$ and $B_0$.  
  
  - $L_0 = b_0$, the intercept from a simple regression equation.  
  
  - $B_0 = b_1$, the slope from a simple regression equation.  
  

### What needs to be Determined/Optimized for? [2]

- The value of the smoothing constant for the level, $\alpha$, and the smoothing constant for the trend, $\beta$.\footnotemark  

  - $0 < \alpha < 1$, and $0 < \beta < 1$;  
  
  - The values for $\alpha$ and $\beta$ may be chosen to be the same or different, depending on the nature of the time series.  
  
  - Often the choices of the smoothing constants are arbitrary.  
  
  - $\alpha$ and $\beta$ can also be chosen by minimizing the mean squared one-step ahead forecast error (MSE) or equivalently, the square root of the mean squared one-step ahead forecast error (RMSE).

\footnotetext{The past four slides are adapted from \href{https://miamioh.edu/fsb/directory/?up=/directory/farmerl2}{Dr. Allison Jones-Farmer's Handouts} for ISA 444, Spring 2020.}

### Example 1: Weekly Thermometer Sales (Chart)

Below is a simple line plot based on ``Weekly_Therm_Sales.xlsx''. 

```{r thermSales, echo=FALSE, results='asis', fig.height=2.5}
thermSales = readxl::read_excel("Data/Weekly_Therm_Sales.xlsx")

thermSales %>% ggplot(aes(x = Time, y = WeeklyThermSales)) + 
  geom_line() + geom_point() + theme_bw()
```


### Example 1: Weekly Thermomemter Sales (By "Hand") [1]

Let us use the first 26 points in the dataset to estimate both $L_0$ and $B_0$.
```{r thermSalesReg, results='asis'}
Time = thermSales$Time[1:26]
WeeklySales = thermSales$WeeklyThermSales[1:26]
regModel = lm(WeeklySales ~ Time)
print(xtable(summary(regModel)$coefficients, align = c(rep('c', 5)), 
             digits = c(0, rep(3, 4)) ), comment = FALSE)
```


### Example 1: Weekly Thermomemter Sales (By "Hand") [2]
Based on the information in the previous slide, please fill the table below. For the purpose of our example, please use $\alpha = 0.2$ and $\beta = 0.1$. Please create and fill this table in the Excel file.
```{r thermSalesTable, echo=FALSE, results='asis'}
thermSales[53, 1:2] = list(0, NA)
thermSales %<>% arrange(Time)
thermSales$Level = "..."
thermSales$Trend = "..."
thermSales$`1-step ahead Forecast` = "..."
print(xtable(thermSales[1:9, ], align = c(rep('c', 6))), comment = FALSE, row.names = FALSE)
```


### Example 1: Using R to Compute the Forecast [1]

**This is a live class demo, where we will use R to obtain the results shown in the next 3 slides.**

```{r holtTherm1, echo=FALSE, fig.height=2.35}
thermSales = readxl::read_excel("Data/Weekly_Therm_Sales.xlsx")
weeklySales = thermSales$WeeklyThermSales
les = holt(weeklySales, alpha = 0.2, beta = 0.1, h=10)
autoplot(les) + theme_bw()
```

### Example 1: Using R to Compute the Forecast [2]

```{r holtTherm2, echo=FALSE, results="asis"}
thermSales$Forecast = les$fitted
print(xtable(thermSales[1:9, ], align = c(rep('c', 4))), comment = FALSE, row.names = FALSE)
```

### Example 1: Using R to Compute the Forecast [3]
```{r holtTherm3, echo=FALSE, results="asis"}
print(xtable(accuracy(les), align = c(rep('c', 8)), digits = c(0, rep(3,7)) ), comment = FALSE)
```

### Optimizing the Smoothing Parameter: WFJ Sales Series
To illustrate the aforementioned concepts, let us examine the data for the [WFJ Sales Example](https://github.com/fmegahed/businessForecasting/raw/master/lectures/09%20-%20Forecasting%20Non-Seasonal%20Series/Data/WFJ_sales.xlsx) (i.e., Example 3.2 in our textbook). Per the textbook example, we will use the first the 26 observations as the estimation sample. **Note that we will now apply LES instead of the SES approach we examined last class (and today).**

```{r wfjSalesLES}
pacman::p_load(readxl)
download.file("https://github.com/fmegahed/businessForecasting/blob/master/assignments/WFJ_sales.xlsx?raw=true", destfile = "Data/WFJ_sales.xlsx", mode = "wb")
WFJ = read_excel("Data/WFJ_sales.xlsx") %>% select(c(1,2))
```

**This example will be coded live in class to obtain the results below.**


```{r wfjSalesCompleteLES, echo=FALSE, results='asis'}
trainData = WFJ[1:26,] # using the first 26 observations for training
ntrain = nrow(trainData) # getting the sample size

les = holt(trainData$`WFJ Sales`, h = 36)
metrics = accuracy(les)
cat(paste0('The optimal alpha and beta obtained using R are equal to ', 
           round(les$model$par['alpha'], 3), ', and ', round(les$model$par['beta'], 3), ', respectively.'), '\n')
print(xtable(metrics, align = c(rep('c', 8)), digits = c(0, rep(3, 7)) ), comment = FALSE)
```

### The Validation Results: The Basics
**A continuation of the live coding session, where we: (a) examine the validation results; and (b) print the combined training and validation results in one table.**

```{r wfjSalesCompleteLES2, echo=FALSE, results='asis'}
lesValid =holt(WFJ$`WFJ Sales`, h =10, alpha = les$model$par['alpha'], beta = les$model$par['beta'])
WFJ$lesOpt = lesValid %>% .[['fitted']]
validationData = WFJ[27:62,]
validationMetrics = accuracy(validationData$`WFJ Sales`, validationData$lesOpt)

combinedMetrics = rbind(metrics[1, -c(6,7)], validationMetrics)
row.names(combinedMetrics) = c('Training Set', 'Validation Set')
print(xtable(combinedMetrics, align = c(rep('c', 6)), digits = c(0, rep(3, 5)) ), comment = FALSE)
```


### The Validation Results: Visually

```{r wfjSalesCompleteLES3, echo=FALSE}
autoplot(lesValid) + autolayer(fitted(lesValid), series = 'LES (optimal)') +
  theme_bw() + theme(legend.position = 'bottom') + 
  labs(x = 'Obs. Number', y = 'Weekly Sales')
```


### The Validation Results: With No Updating (Based on Training Model)

```{r wfjSalesCompleteLES4, echo=FALSE, fig.height=2.4}
autoplot(les) + 
  autolayer(fitted(les), series = 'LES (optimal)') +
  theme_bw() + theme(legend.position = 'bottom') + 
  labs(x = 'Obs. Number', y = 'Weekly Sales')
```



# Recap

### Summary of Main Points

\begin{block}{\textbf{Main Learning Outcomes}}
  \begin{itemize}
			\item \textbf{Recognize time series that are appropriate for simple exponential smoothing (SES).}
			\item \textbf{Use SES to smooth past observations of a time series.}
			\item \textbf{Use SES to forecast future observations of a time series.}
			\item \textbf{Compare the forecasting performance of SES to other suitable techniques (i.e., methods that require similar assumptions).}
			\item \textbf{Recognize time series that are appropriate for linear exponential smoothing (LES).}
			\item \textbf{Use LES to forecast future observations of a time series.}
	\end{itemize}
\end{block}


### Things to Do

 - **Recommended:** Thoroughly read Chapter 3.1-3.4 of our book.  
 
 - Go through the slides, examples and make sure you have a good understanding of what we have covered.  
 
 - **Highly Recommended:** Go through the [Week 04-05 Self-Paced Study Guide](rstudio.fsb.miamioh.edu:3838/megahefm/isa444/weeks04-05/).  

- **Required:** Complete the two graded assignments (see details in next slides).
 

### Graded Assignment 07: Evaluating your Understanding

Please go to \href{https://miamioh.instructure.com/courses/142177/quizzes/370024}{Canvas (click here)} and answer the questions. **Due March 01, 2021 [11:40 AM, Ohio local time]** 

**What/Why/Prep?** The purpose of this assignment is to evaluate your understanding and retention of the material covered up to the end of Class 09. To reinforce your understanding of the covered material, I also suggest reading Chapter 3.1-3.3 of the book.

**General Guidelines:**  

\vspace{-0.5\baselineskip}

  - Individual assignment.  
  - This is **NOT** a timed assignment.    
  - Proctorio is NOT required for this assignment.  
  - You will need to have R installed (or accessible through the \href{https://virtualpc.fsb.miamioh.edu/RDWeb/Pages/en-US/default.aspx}{Remote Desktop})


### Graded Assignment 08: Evaluating your Understanding

Please go to \href{https://miamioh.instructure.com/courses/142177/quizzes/371145}{Canvas (click here)} and answer the questions. **Due March 01, 2021 [11:40 AM, Ohio local time]** 

**What/Why/Prep?** The purpose of this assignment is to evaluate your understanding and retention of Holt's method. To reinforce your understanding of the covered material, I also suggest reading Chapter 3.1-3.4 of the book.

**General Guidelines:**  

\vspace{-0.5\baselineskip}

  - Individual assignment.  
  - This is **NOT** a timed assignment.    
  - Proctorio is NOT required for this assignment.  
  - You will need to have R installed (or accessible through the \href{https://virtualpc.fsb.miamioh.edu/RDWeb/Pages/en-US/default.aspx}{Remote Desktop})


---

\maketitle